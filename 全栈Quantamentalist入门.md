# 全栈Quantamentalist入门

### 背景

实习刚入职的时候团队连我四个人，基本面背景向Quantamental转型。有多个数据供应商，没有统一的数据供应渠道也没有数据库，全部靠第三方Excel模板或者邮件链接下载。大部分基本面PM是在这样的环境配置下工作的，靠着娴熟的Excel技能和个人能力业绩也很好。

虽然业务能力和基础设施并没有太直接的关系，然而完整清晰的工作流以及用户友好的环境还是很有助于提高工作效率的。对于fundamental analyst来说，用户友好的交互界面可以省去大把在Excel里做ETL的时间从而可以专注于分析；对于data analyst来说，规范格式的数据和简单统一的数据源是后期开展各类建模的基础。

所以本文希望总结一下过去几个月做的一些从data engineer到web developer到data scientist再到quant researcher的工作（一个入门级full stack data analyst），反思一下技术不到位的地方以及思路上的缺陷；如果说模型、分析和策略是道，那么这篇文章更侧重于术，讨论量化研究和数据分析的模块设计以及工程实现。

##### 适用人群

startup团队，包括创业量化团队和基本面基金的量化转型团队；对后台技术支持完整的数据分析师来说，深入自己的研究领域比做一个全栈可能更重要，但也可以借此简单了解一下其他环节的工作。

##### 模块

1. Data pipeline
2. Web platform
3. Quant trading

#### Data pipeline

目的：规范化数据获取的方式，统一数据格式，统一读写到自己的数据库。在之后的所有模块中，避免与数据源发生直接的交互而使用自己的数据库，一方面省去数据清洗步骤，另一方面提高安全性和稳定性。

技术：

1. 命令行：包括基本命令、python编译、以及如果有供应商提供命令行API也需要会使用
2. SQL及各种关系型数据库：从下载到安装到连接到新建script再到select
3. 基础http：对GET/POST方法和JSON格式有基本了解
4. 上述技术的python实现及交互，和python数据处理：
   - 用subprocess调用命令行
   - 用requests包（比urllib好用）发送请求获取相应
   - 用numpy和pandas清洗数据
   - 调用各种SQL数据库的接口，进阶可以使用SQLAlchemy做ORM，以实现数据库读写

反思：

1. 是否实现高度模块化？不同的数据源有不同的原始格式和获取方法，但是对于标准化后的数据来说，数据库的读写方法应该比较一致，可以重复使用的代码应该独立于数据源，通过继承基础类、覆写部分方法的方式实现数据源的模块化，提高代码重复使用性以及数据源的可扩展性（更快地接收新供应商的数据）
2. 模块的设计是否合理？如上述所说按数据源分模块比较直观，但对于同一数据源不同数据集（比如BBG）如果使用单一模块会显得太过臃肿。是否可以在这一模块下继续分出子模块，共享接口但覆写数据获取及清洗的部分？
3. 数据安全性够高吗？这里的安全性不针对外部攻击，特指由于自己误操作导致的数据丢失等问题，因此着重考虑的应该包括更新数据时覆盖的历史部分、不同knowledge date的切片存储、以及必要的冗余备份
4. 效率是否够高？用python的好处是开发难度相对较低，容易在短时间内做出比较完整的工作流，但作为底层支持，如果后期数据量上升且线程变多，是否需要用其他语言重构，并且引入MapReduce？

#### Web platform

目的：主要为基本面分析服务

1. 数据表及可视化：包括源数据、财务数据、公司信息、财务日历等等
2. 数据分析和预测：将常用的一些分析模型和预测模型模块化，通过前端用户交互提交自定义参数在后端进行分析和计算，并将结果呈现在前端
3. 其他常用信息的整合及呈现：团队非常常用但分散在不同地方的信息都可以整合进来做一个定制的dashboard，比如portfolio profile等

技术：基于python的网络开发

1. 后端：个人偏好Django，简单开发速度快功能全
2. 前端：利用Django Jinja2模板，基本css+jQuery搞定一切；做可视化会需要一些基本js技能，[D3.js][d3] 非常之强大但是门槛比较高，这里比较推荐[highcharts][highcharts]，包含了各类基本图且其highstocks模块尤其适合金融时间序列
3. 数据及基本面分析：后端分析模型的核心，分成两部分
   - 基本面分析：准确定位公司行业、类型及核心KPI，定位和KPI相关的数据集，从供应商处了解清楚数据的含义、定义以及采集方式（比如credit card和debit card数据采集的区别，大集团下不同concept的tracking index的区别等）从而建立基本的模型
   - 数据处理和分析：通过ORM做ETL，进一步根据模型进行处理（平滑、重新采样、变频率指数化），建模进行分析和预测，报告结果以及对结果的评估指标

反思：

1. 前端丑不丑？确实不好看。。。对css语法不熟悉导致对单个部分做调整时候不必要得影响到了其他部分，响应式在不同屏幕尺寸下的没有达到预期的效果；更重要的是，在交互的部分需要更多的调整，针对参数格式不同需要更合理的表单选择和排布，js的部分也需要提高可重复使用性
2. 模型模块化是否到位？首先避免代码重复，其次通过模块化可以在view中调用模型对象，简化view的结构
3. view的设计是否合理？以Django为例，有对象化和函数化两种方式，分别适用于怎样的场景？
4. 缓存怎么设计？模型复杂计算变多直接导致页面加载变慢，缓存可以让已访问页面加载更快；但是对于不同的用户输入参数，缓存应该怎样设计？服务器端缓存和客户端缓存适用于怎样的情况？

#### Quant trading

目的：基本的量化交易信号生成器；由于合规问题交易不用自己下单，所以需要生成结构化交易指令数据传递给交易员或者算法

技术：

1. 工作流设计：数据 -> 信号 -> 目标仓位 -> 实际交易，设计信号、策略及风控模块，明确每个模块的输入和输出
2. 信号研究，策略回测，风险控制等传统量化交易技术

反思：

1. 信号研究和策略研究是否能/需要分开？一个策略的交易信号可以有多个数据信号复合而成，每个数据信号又由不同数据集不同模型计算，那么在信号研究模块需要对单个信号也进行评估和分析，在挑选出有效信号之后再进一步进行更完整的策略设计和回测；优点在于可以对信号质量进行快速分析，尽快排除一些完全没价值的信号，缺点在于可能会错过一些信号。因此，基本面分析和解释能力在此处尤为重要；信号研究不仅需要bottom up地进行数据挖掘和统计建模，同时也需要有top down的基本面原理解释，在两重标准下进行筛选
2. 对于数据频率更高的策略，如何设计persistent sessions或者通过多线程进行实时的data feed？中低频策略可以定时跑一下，检验trigger或者进行rebalance，但如果策略频率更高则需要实时的数据、信号、目标仓位信息流，实现会更为复杂

### TIPs

有一些统计或数据科学背景的朋友想转行，大多在学校接受了系统性课程学习，但是没有实际实习或者工作的经验，因此对数据分析的研究层面有理论基础和一定的编程实现能力，但对工程化和产品化没有太多的认识。观察到的一些常见注意点包括

1. 配置环境：这个大概是最不起眼然而对入门最不友好的环节，认识很多python从安装到放弃的朋友，SQL数据库能熟练SELECT但是换一台新电脑不知道怎么连上的更多。对于data scientist来说可能这些环境都是配置好的（就像在kaggle里直接能做的那样），实际工作中也可以让IT support把所有的东西都下载安装连接好，然而在这一步受到技术制约太过浪费时间，而且也会因为沟通问题IT没法把电脑配成你需要的样子，所以**非常强烈建议**入门技术时从学会环境配置开始
2. 区分REST API和爬虫：其实不是什么太重要的问题，技术上还有一些重叠，但compliance问起来的时候你要是说你在用爬虫怕是要被盘问死。。。
3. 使用命令行：主要指利用命令行编译.py文件而不是把所有的内容写在.ipynb里；从单纯研究的角度只会.ipynb也不是不可以，做完了研究总有别人替你产品化，然而对全栈数据分析师来说，尽量别在这么初始的阶段受制于人
4. 解释模型：理工科背景转行做Quantamental的短板通常不在quant而在fundamental，模型一旦复杂，每一步的逻辑、每一个参数的意义都会更难解释，模型的置信度就会有一定的下降；像一个学长告诉我的，模型不能做太复杂否则很容易过拟合，要在了解数据和基本面含义的情况下“将拍大腿做决定发挥到极致”

### 总结

要做和改善的还有很多，重新整理思路、重构代码和上述技术相比应该是最重要的技能；一时设计的缺陷（大多是因为一时偷懒所以延展性没做好）需要用不断的反思和重构来弥补并且提升。

设计和代码是术，模型思路和策略是道，术的完善是为了在研究道的时候有更强大的平台和技术支持，而最终的目的还是在Quantamental道的方向上走得更远，做出更合理、准确、精确的分析和预测，然后在交易中盈利。

[d3]: https://d3js.org/
[highcharts]: https://www.highcharts.com/